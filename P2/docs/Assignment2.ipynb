{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m05O2ng5C0Q3"
      },
      "source": [
        "# Natural Language Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfHhrISOAeQW"
      },
      "source": [
        "## Original Dataset\n",
        "\n",
        "Natural Language Processing using a dataset of Statements and their respective Emotion: each statement is either fear, anger or joy. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0YKb71qEDhl"
      },
      "source": [
        "Check if the dataset is being read properly and if values match, after removing duplicate data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUNkON6wB7YD",
        "outputId": "2d93ad32-5f46-4fc7-fdfd-7bbae15bca69"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Importing the dataset\n",
        "dataset = pd.read_csv('Emotions.csv', encoding='cp1252')\n",
        "dataset.drop_duplicates(subset=\"Statement\", keep='first', inplace=True)\n",
        "\n",
        "# Printing the statements\n",
        "for index, row in dataset.iterrows():\n",
        "  print(row['Statement'])\n",
        "\n",
        "# Check if everything looks alright\n",
        "print('\\nDataset size:\\n')\n",
        "print(dataset['Statement'].size)\n",
        "print('\\nFear count:\\n')\n",
        "print((dataset['Emotion'] == 'fear').value_counts(normalize=True))\n",
        "dataset_fear = [(dataset['Emotion'] == 'fear')]\n",
        "print('\\nAnger count:\\n')\n",
        "print((dataset['Emotion'] == 'anger').value_counts(normalize=True))\n",
        "dataset_anger = [(dataset['Emotion'] == 'anger')]\n",
        "print('\\nJoy count:\\n')\n",
        "print((dataset['Emotion'] == 'joy').value_counts(normalize=True))\n",
        "dataset_joy = [(dataset['Emotion'] == 'joy')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRPa39kTBAzO"
      },
      "source": [
        "## Clean Up and Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m09fHf6lAMvW"
      },
      "source": [
        "As only around 4% of the words are in upper case, we consider it is not important to keep for the training of the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdILiY5KAO02",
        "outputId": "72c216ca-2a8b-4b85-c417-5d9c1dee7e2a"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "upper_counter = 0\n",
        "full_counter = 0\n",
        "\n",
        "for index, row in dataset.iterrows():\n",
        "  # replace asterisk for empty\n",
        "  review = re.sub('\\*', '', row['Statement'])\n",
        "  # remove non alpha chars\n",
        "  review = re.sub('[^a-zA-Z]', ' ', review)\n",
        "  for w in review.split():\n",
        "    if w.isupper():\n",
        "      upper_counter += 1\n",
        "    full_counter += 1\n",
        "\n",
        "print((upper_counter / full_counter) * 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAxvwERFFotQ"
      },
      "source": [
        "Removing non alpha chars from the statements, lowercasing, stopword removal and stemming:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZPewmacF8m9",
        "outputId": "a8d3f0e3-cc8b-48d4-e791-d019ba8b0f60"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "stopwords = list(set(stopwords.words('english')))\n",
        "\n",
        "# Add word\n",
        "stopwords.append('amp')\n",
        "\n",
        "# Remove negations\n",
        "filter = \"'t\"\n",
        "for word in stopwords:\n",
        "  if filter in word:\n",
        "    stopwords.remove(word)\n",
        "stopwords.remove('not')\n",
        "stopwords.remove('no')\n",
        "\n",
        "print(stopwords)\n",
        "corpus = []\n",
        "ps = PorterStemmer()\n",
        "for index, row in dataset.iterrows():\n",
        "    # replace asterisk for empty\n",
        "    review = re.sub('\\*', '', row['Statement'])\n",
        "    # remove non alpha chars\n",
        "    review = re.sub('[^a-zA-Z]', ' ', review)\n",
        "    # to lower case\n",
        "    review = review.lower()\n",
        "    # split into tokens, apply stemming and remove stop words\n",
        "    review = ' '.join([ps.stem(w) for w in review.split() if not w in stopwords])\n",
        "    corpus.append(review)\n",
        "\n",
        "print(corpus)\n",
        "print(len(corpus))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It5GeZ5EBR4A"
      },
      "source": [
        "## Wordclouds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGD05vTp9XKy"
      },
      "source": [
        "Generating the global wordcloud:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "raJM7LfM9ZWm",
        "outputId": "1a81d48e-85c7-4359-e2f0-c9ba007144a4"
      },
      "outputs": [],
      "source": [
        "%pip install wordcloud\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "wordcloud = WordCloud().generate(\" \".join(corpus))\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBhGh0Cm3cap"
      },
      "source": [
        "Generating wordcloud for fear:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "dj5z-w0n3irp",
        "outputId": "fae5f772-0378-43fa-9f6e-dc8d0e18f6ef"
      },
      "outputs": [],
      "source": [
        "corpus_fear = []\n",
        "\n",
        "for index, row in dataset.iterrows():\n",
        "  if row['Emotion'] == 'fear':\n",
        "    corpus_fear.append(corpus[index])\n",
        "\n",
        "wordcloud = WordCloud().generate(\" \".join(corpus_fear))\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAJIUPDq7CHX"
      },
      "source": [
        "Generationg wordcloud for anger:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "EsAisplZ7Fnb",
        "outputId": "c1ac9454-8712-46ea-b158-fee94a071574"
      },
      "outputs": [],
      "source": [
        "corpus_anger = []\n",
        "\n",
        "for index, row in dataset.iterrows():\n",
        "  if row['Emotion'] == 'anger':\n",
        "    corpus_anger.append(corpus[index])\n",
        "\n",
        "wordcloud = WordCloud().generate(\" \".join(corpus_anger))\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBOruA1S7O2D"
      },
      "source": [
        "Generating wordcloud for joy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "XIb62boS7SLK",
        "outputId": "8e66fc5e-30f8-4244-d3c2-839104374bc0"
      },
      "outputs": [],
      "source": [
        "corpus_joy = []\n",
        "\n",
        "for index, row in dataset.iterrows():\n",
        "  if row['Emotion'] == 'joy' and index < dataset['Statement'].size: \n",
        "    corpus_joy.append(corpus[index])\n",
        "\n",
        "wordcloud = WordCloud().generate(\" \".join(corpus_joy))\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z3iIzskBZzt"
      },
      "source": [
        "## Generating a Dataset \n",
        "\n",
        "We need to transform the data into a dataset that can be used by machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KBfeLxmFcBH"
      },
      "source": [
        "We can choose scikit-learn's Bag of Words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DH3ZuNPLEBnA"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus).toarray()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeao7kOcWIkG"
      },
      "source": [
        "Or scikit-learn's TF-IDF:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uzBdCAxWSfZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xgv-R2wWqyE"
      },
      "source": [
        "We can also use N-grams, which is useful for negations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOHvDAu5V7GC"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
        "X = vectorizer.fit_transform(corpus).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOle14X4W_Gm"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
        "X = vectorizer.fit_transform(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EncJpYimXKNj"
      },
      "source": [
        "Look at shape and features we've got:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtY4irWJXHaw",
        "outputId": "45eedcba-a535-498e-b7f6-58439d5633da"
      },
      "outputs": [],
      "source": [
        "print(X.shape)\n",
        "print(vectorizer.get_feature_names())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV8YVHDOF0dk"
      },
      "source": [
        "Compare the contents of one review with its representation vector following the bag-of-words model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8--jv1_2GA9Q",
        "outputId": "7ac4923c-b46e-4a09-9632-3657735d5d10"
      },
      "outputs": [],
      "source": [
        "print(dataset['Statement'][999])\n",
        "print(corpus[999])\n",
        "print(X[999])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ld4-QUYGXcS"
      },
      "source": [
        "Obtaining the classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2-vHUdTHCyM",
        "outputId": "6ed82e59-3bfa-4a01-d4a2-264d0eb53fbb"
      },
      "outputs": [],
      "source": [
        "y = dataset['Emotion']\n",
        "print(X.shape, y.shape) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbjFjlnxK5Gz"
      },
      "source": [
        "## Training Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7H5YjwfK7u4",
        "outputId": "f82cf2b6-f03a-4a87-e3ea-390052156a6c"
      },
      "outputs": [],
      "source": [
        "print(\"\\nLabel distribution in the training set:\")\n",
        "print(y.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTZcq88sPeWf"
      },
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOdrv1b_P0D6",
        "outputId": "17d7c6ba-47ed-40ac-a2ac-2980dde226c3"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by93KPp4EYwS"
      },
      "source": [
        "### Decision Trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kLznOx1EcRm",
        "outputId": "fcd5e54a-1f4c-48c5-fe45-7d98526e44a2"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "clf = DecisionTreeClassifier(random_state=0)\n",
        "clf.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7hKMAwKz4cR"
      },
      "source": [
        "### Support Vector Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nKVbiHTz6u6",
        "outputId": "75784c12-d2eb-4e69-b361-e54380a2461b"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "clf = SVC() # can add probability=True but it will take longer\n",
        "clf.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3vtVV5an1a0",
        "outputId": "99b7698d-b4f5-404f-865e-3c3ed2527999"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "clf = RandomForestClassifier(random_state=0)\n",
        "clf.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayCPCJKbsOru"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XHEmoefGJL9"
      },
      "source": [
        "### Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKzHfur9GNCC",
        "outputId": "e616706e-2421-46e6-db40-9076ee6c59e8"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "# can increase the cv parameter but it will take longer\n",
        "scores = cross_validate(clf, X, y, scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'], cv=3, return_train_score=True)\n",
        "\n",
        "print(scores)\n",
        "print(scores['test_accuracy'].mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvRUMbPtQzH3"
      },
      "source": [
        "## Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94sYLZn3Gnht"
      },
      "source": [
        "Building the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2BdI_w8Q3nN",
        "outputId": "611d06ea-13a3-485e-8165-5ba399796bdb"
      },
      "outputs": [],
      "source": [
        "test_set = pd.read_csv('Testingdata.csv', encoding='cp1252')\n",
        "\n",
        "test_corpus = []\n",
        "for index, row in test_set.iterrows():\n",
        "  # replace asterisk for empty\n",
        "  review = re.sub('\\*', '', row['Statement'])\n",
        "  # remove non alpha chars\n",
        "  review = re.sub('[^a-zA-Z]', ' ', review)\n",
        "  # to lower case\n",
        "  review = review.lower()\n",
        "  # split into tokens, apply stemming and remove stop words\n",
        "  review = ' '.join([ps.stem(w) for w in review.split() if not w in stopwords])\n",
        "  test_corpus.append(review)\n",
        "\n",
        "X_test = vectorizer.transform(test_corpus).toarray()\n",
        "y_test = test_set['Emotion']\n",
        "print(X_test.shape, y_test.shape)\n",
        "print(\"\\nLabel distribution in the testing set:\")\n",
        "print(y_test.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N18JHftvSjWF"
      },
      "source": [
        "Let's see the model's output on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qP9sW0ZnSl5d",
        "outputId": "29b9d386-b1fc-4ff3-a60e-f8db31f1375e"
      },
      "outputs": [],
      "source": [
        "%pip install colorama\n",
        "from colorama import Fore\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "for i in range(0, len(y_pred)):\n",
        "  if (y_pred[i] == y_test[i]):\n",
        "    print(Fore.GREEN + y_pred[i])\n",
        "  else:\n",
        "    print(Fore.RED + y_pred[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rog3ZieyXPBS"
      },
      "source": [
        "Assess the performance of our model by looking at different metrics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "zyj8v73qXT0d",
        "outputId": "cc574117-8ba9-4dc7-dd01-7018749782f4"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# confusion matrix\n",
        "labels = [\"anger\", \"fear\", \"joy\"]\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cmd_obj = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
        "cmd_obj.plot()\n",
        "cmd_obj.ax_.set(\n",
        "                title='Confusion Matrix', \n",
        "                xlabel='Predicted Emotion', \n",
        "                ylabel='Actual Emotion')\n",
        "print(\"Confusion matrix:\")\n",
        "plt.show()\n",
        "\n",
        "# accuracy\n",
        "print(\"\\nAccuracy:\") \n",
        "print(accuracy_score(y_test, y_pred))\n",
        "\n",
        "# precision\n",
        "print(\"\\nPrecision:\")\n",
        "print(precision_score(y_test, y_pred, labels=labels, average=None)) \n",
        "\n",
        "# recall\n",
        "print(\"\\nRecall:\")\n",
        "print(recall_score(y_test, y_pred, labels=labels, average=None)) \n",
        "\n",
        "# f1\n",
        "print(\"\\nf1:\")\n",
        "print(f1_score(y_test, y_pred, labels=labels, average=None)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYS3OOK-Qa-1"
      },
      "source": [
        "## Try it Yourself"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzegXtA3QgRt",
        "outputId": "29c98b5d-8bad-4f22-fe40-a08da29a90b3"
      },
      "outputs": [],
      "source": [
        "your_statement = \"I am happy\" \n",
        "\n",
        "# replace asterisk for empty\n",
        "review = re.sub('\\*', '', your_statement)\n",
        "# remove non alpha chars\n",
        "review = re.sub('[^a-zA-Z]', ' ', review)\n",
        "# to lower case\n",
        "review = review.lower()\n",
        "# split into tokens, apply stemming and remove stop words\n",
        "review = ' '.join([ps.stem(w) for w in review.split() if not w in stopwords]) \n",
        "\n",
        "V = vectorizer.transform([review]).toarray()\n",
        "\n",
        "print(V.shape)\n",
        "print(V)\n",
        "print(clf.predict(V))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIq1tiQMWWD9"
      },
      "source": [
        "If the classifier has it you can check probability:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hd2MaldiWdoh",
        "outputId": "6dd53a0a-549f-443b-d5a0-213b4b0f2af7"
      },
      "outputs": [],
      "source": [
        "print(clf.classes_)\n",
        "print(clf.predict_proba(V))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Assignment2.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "783337f819fea5acc09ea1acd9d1467db8ed9d914477ee55454335b84262e76b"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
